{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/liaowenjie/桌面/多模态大模型/lmms-finetune/eval/score\n"
     ]
    }
   ],
   "source": [
    "cd /home/liaowenjie/桌面/多模态大模型/lmms-finetune/eval/score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading qwen-vl-chat from base model ../../../models/Qwen-VL-Chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heads': 16, 'image_size': 448, 'image_start_id': 151857, 'layers': 48, 'mlp_ratio': 4.9231, 'output_dim': 4096, 'patch_size': 14, 'width': 1664}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 10/10 [00:03<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model in torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from builder import load_pretrained_model\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_path = model_base = '../../../models/Qwen-VL-Chat'\n",
    "model_name = 'Qwen-VL-Chat'\n",
    "model, processor, tokenizer, config = load_pretrained_model(\n",
    "    model_path, model_base, model_name, device='cuda'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are you?\n",
      "(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'))\n",
      "(tensor([[ 5.0938,  9.6250,  9.5625,  ..., -0.3125, -0.3125, -0.3125]],\n",
      "       device='cuda:0'), tensor([[ 7.6250,  8.5000,  4.4688,  ..., -1.6875, -1.6875, -1.6875]],\n",
      "       device='cuda:0'), tensor([[ 2.2500,  2.8750,  1.3984,  ..., -2.3125, -2.3125, -2.3125]],\n",
      "       device='cuda:0'), tensor([[ 1.9531,  3.9219,  0.3867,  ..., -2.3594, -2.3594, -2.3594]],\n",
      "       device='cuda:0'), tensor([[ 2.5781,  2.0312,  1.3125,  ..., -2.1719, -2.1719, -2.1719]],\n",
      "       device='cuda:0'), tensor([[ 5.5312,  1.7656,  4.0625,  ..., -2.9375, -2.9375, -2.9375]],\n",
      "       device='cuda:0'), tensor([[ 6.6250,  2.8594, -3.0625,  ..., -3.5781, -3.5781, -3.5781]],\n",
      "       device='cuda:0'), tensor([[ 4.9375,  3.0469,  0.3379,  ..., -2.2969, -2.2969, -2.2969]],\n",
      "       device='cuda:0'), tensor([[ 2.3906,  3.3594, -0.0354,  ..., -3.3125, -3.3125, -3.3125]],\n",
      "       device='cuda:0'), tensor([[ 5.4375,  1.1484,  0.4062,  ..., -3.2344, -3.2344, -3.2344]],\n",
      "       device='cuda:0'), tensor([[ 9.2500,  3.1875,  0.7266,  ..., -3.0469, -3.0469, -3.0469]],\n",
      "       device='cuda:0'), tensor([[-2.9375, -9.7500, -7.5938,  ..., -1.0078, -1.0078, -1.0078]],\n",
      "       device='cuda:0'), tensor([[ 5.1250,  8.3125,  5.3125,  ..., -1.0859, -1.0859, -1.0859]],\n",
      "       device='cuda:0'), tensor([[ 4.1562,  3.8125,  0.6953,  ..., -1.3516, -1.3516, -1.3516]],\n",
      "       device='cuda:0'), tensor([[ 1.6094,  4.3125, -1.8594,  ..., -2.8594, -2.8594, -2.8594]],\n",
      "       device='cuda:0'), tensor([[ 3.7656,  1.1797,  5.1250,  ..., -1.0625, -1.0625, -1.0625]],\n",
      "       device='cuda:0'), tensor([[ 5.5312,  2.2500,  0.7266,  ..., -3.2656, -3.2656, -3.2656]],\n",
      "       device='cuda:0'), tensor([[ 1.4062,  1.4844,  2.3281,  ..., -2.0312, -2.0312, -2.0312]],\n",
      "       device='cuda:0'), tensor([[ 8.6250,  1.3203,  1.7891,  ..., -4.9375, -4.9375, -4.9375]],\n",
      "       device='cuda:0'), tensor([[ 0.0679, -3.4844, -1.0312,  ..., -2.8750, -2.8750, -2.8750]],\n",
      "       device='cuda:0'), tensor([[3.2812, 3.4219, 2.0781,  ..., 1.2109, 1.2109, 1.2109]],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "from typing import List, Tuple\n",
    "def get_stop_words_ids(chat_format, tokenizer):\n",
    "    if chat_format == \"raw\":\n",
    "        stop_words_ids = [tokenizer.encode(\"Human:\"), [tokenizer.eod_id]]\n",
    "    elif chat_format == \"chatml\":\n",
    "        stop_words_ids = [[tokenizer.im_end_id], [tokenizer.im_start_id]]\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown chat format {chat_format!r}\")\n",
    "    return stop_words_ids\n",
    "def make_context(\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    query: str,\n",
    "    history: List[Tuple[str, str]] = None,\n",
    "    system: str = \"\",\n",
    "    max_window_size: int = 6144,\n",
    "    chat_format: str = \"chatml\",\n",
    "):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    if chat_format == \"chatml\":\n",
    "        im_start, im_end = \"<|im_start|>\", \"<|im_end|>\"\n",
    "        im_start_tokens = [tokenizer.im_start_id]\n",
    "        im_end_tokens = [tokenizer.im_end_id]\n",
    "        nl_tokens = tokenizer.encode(\"\\n\")\n",
    "\n",
    "        def _tokenize_str(role, content):\n",
    "            return f\"{role}\\n{content}\", tokenizer.encode(\n",
    "                role, allowed_special=set(tokenizer.IMAGE_ST)\n",
    "            ) + nl_tokens + tokenizer.encode(content, allowed_special=set(tokenizer.IMAGE_ST))\n",
    "\n",
    "        system_text, system_tokens_part = _tokenize_str(\"system\", system)\n",
    "        system_tokens = im_start_tokens + system_tokens_part + im_end_tokens\n",
    "\n",
    "        raw_text = \"\"\n",
    "        context_tokens = []\n",
    "\n",
    "        for turn_query, turn_response in reversed(history):\n",
    "            query_text, query_tokens_part = _tokenize_str(\"user\", turn_query)\n",
    "            query_tokens = im_start_tokens + query_tokens_part + im_end_tokens\n",
    "            if turn_response is not None:\n",
    "                response_text, response_tokens_part = _tokenize_str(\n",
    "                    \"assistant\", turn_response\n",
    "                )\n",
    "                response_tokens = im_start_tokens + response_tokens_part + im_end_tokens\n",
    "\n",
    "                next_context_tokens = nl_tokens + query_tokens + nl_tokens + response_tokens\n",
    "                prev_chat = (\n",
    "                    f\"\\n{im_start}{query_text}{im_end}\\n{im_start}{response_text}{im_end}\"\n",
    "                )\n",
    "            else:\n",
    "                next_context_tokens = nl_tokens + query_tokens + nl_tokens\n",
    "                prev_chat = f\"\\n{im_start}{query_text}{im_end}\\n\"\n",
    "\n",
    "            current_context_size = (\n",
    "                len(system_tokens) + len(next_context_tokens) + len(context_tokens)\n",
    "            )\n",
    "            if current_context_size < max_window_size:\n",
    "                context_tokens = next_context_tokens + context_tokens\n",
    "                raw_text = prev_chat + raw_text\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        context_tokens = system_tokens + context_tokens\n",
    "        raw_text = f\"{im_start}{system_text}{im_end}\" + raw_text\n",
    "        context_tokens += (\n",
    "            nl_tokens\n",
    "            + im_start_tokens\n",
    "            + _tokenize_str(\"user\", query)[1]\n",
    "            + im_end_tokens\n",
    "            + nl_tokens\n",
    "            + im_start_tokens\n",
    "            + tokenizer.encode(\"assistant\")\n",
    "            + nl_tokens\n",
    "        )\n",
    "        raw_text += f\"\\n{im_start}user\\n{query}{im_end}\\n{im_start}assistant\\n\"\n",
    "\n",
    "    elif chat_format == \"raw\":\n",
    "        raw_text = query\n",
    "        context_tokens = tokenizer.encode(raw_text)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unknown chat format {chat_format!r}\")\n",
    "\n",
    "    return raw_text, context_tokens\n",
    "\n",
    "stop_words_ids = []\n",
    "prompt = [\n",
    "    {'text': \"Who are you?\"},\n",
    "]\n",
    "query = tokenizer.from_list_format(prompt)\n",
    "print(query)\n",
    "raw_text, context_tokens = make_context(\n",
    "    tokenizer,\n",
    "    query,\n",
    "    max_window_size=8092,\n",
    "    chat_format=\"chatml\",\n",
    ")\n",
    "\n",
    "stop_words_ids.extend(get_stop_words_ids(\n",
    "    \"chatml\", tokenizer\n",
    "))\n",
    "input_ids = torch.tensor([context_tokens]).to(model.device)\n",
    "outputs = model.generate(\n",
    "            input_ids,\n",
    "            stop_words_ids=stop_words_ids,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            output_logits=True\n",
    "        )\n",
    "print(outputs.scores)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are you?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mfrom_list_format(prompt)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(query)\n\u001b[0;32m----> 6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs) \n\u001b[1;32m      8\u001b[0m response, history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    {'text': \"Who are you?\"},\n",
    "]\n",
    "query = tokenizer.from_list_format(prompt)\n",
    "print(query)\n",
    "inputs = processor(text=query, return_tensors=\"pt\")\n",
    "print(inputs) \n",
    "response, history = model.generate(**inputs, max_new_tokens=128)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Conclusion:** The quality of the image is fair.\n",
      "\n",
      "- **Distortion assessment:**\n",
      "  - **Noise:**  \n",
      "    - 1 instance of moderate noise affects the curtains and window details on the left side. It appears as random speckles and graininess, disrupting smoothness and slightly obscuring patterns.  \n",
      "    - 1 instance of minor noise affects the upper part of the window, introducing a mild grainy texture that reduces clarity but does not significantly alter perception.  \n",
      "\n",
      "  - **Meaningless solid color:**  \n",
      "    - 4 instances of extreme meaningless solid color distortions:  \n",
      "      - The bottom area is entirely filled with solid color, erasing visible details and textures, creating a flat, visually disruptive section.  \n",
      "      - The left side is obscured by solid color, creating a void in the composition and erasing potential objects or textures.  \n",
      "      - The top area is covered with solid color, eliminating patterns and details, disrupting continuity.  \n",
      "      - The right side is filled with solid color, erasing details and breaking the visual flow.  \n",
      "\n",
      "  - **Edge aliasing:**  \n",
      "    - 1 instance of moderate edge aliasing affects the central curtain and window edges. The edges appear jagged and exhibit stair-step patterns, diminishing smoothness and clarity.  \n",
      "\n",
      "- **Scene description:** The image depicts a window with semi-transparent curtains that allow sunlight to filter through, creating a warm, golden ambiance in the room\n"
     ]
    }
   ],
   "source": [
    "test = \"- **Conclusion:** The quality of the image is fair.\\n\\n- **Distortion assessment:**\\n  - **Noise:**  \\n    - 1 instance of moderate noise affects the curtains and window details on the left side. It appears as random speckles and graininess, disrupting smoothness and slightly obscuring patterns.  \\n    - 1 instance of minor noise affects the upper part of the window, introducing a mild grainy texture that reduces clarity but does not significantly alter perception.  \\n\\n  - **Meaningless solid color:**  \\n    - 4 instances of extreme meaningless solid color distortions:  \\n      - The bottom area is entirely filled with solid color, erasing visible details and textures, creating a flat, visually disruptive section.  \\n      - The left side is obscured by solid color, creating a void in the composition and erasing potential objects or textures.  \\n      - The top area is covered with solid color, eliminating patterns and details, disrupting continuity.  \\n      - The right side is filled with solid color, erasing details and breaking the visual flow.  \\n\\n  - **Edge aliasing:**  \\n    - 1 instance of moderate edge aliasing affects the central curtain and window edges. The edges appear jagged and exhibit stair-step patterns, diminishing smoothness and clarity.  \\n\\n- **Scene description:** The image depicts a window with semi-transparent curtains that allow sunlight to filter through, creating a warm, golden ambiance in the room\"\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmms-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
